{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ByronFC3/2/blob/main/MC_EI2_Prediction_RAG_Model_18June_28April.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGwUQ87tj0Xq"
      },
      "source": [
        "# Building a RAG System for Analyzing NSF Grants"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The point of the code below is to see if I can plug any PDF into it and use RAG to quary the content of that PDF to answer questions about the document. This is the simulate a companies newly onboarded employee needed to know how to do something on their job and not having access to a manager or co-worker."
      ],
      "metadata": {
        "id": "Pq1Fdwxpuhsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "bY3qFJufiWdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "import joblib\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from getpass import getpass\n",
        "\n",
        "# === Configuration ===\n",
        "# Securely input your API key if it's not already set\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass('Enter your OpenAI API key: ')\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# --- Define your specific PDF for RAG ---\n",
        "MC_GENAI_STUDY_PDF = \"/content/the-economic-potential-of-generative-ai-the-next-productivity-frontier (1).pdf\"\n",
        "print(f\"Default RAG PDF set to: {MC_GENAI_STUDY_PDF}\")\n",
        "\n",
        "# Paths for proposal directories (update as needed) - these are primarily used for training the classifier\n",
        "# Ensure these directories exist and contain some PDFs for the classifier to train on\n",
        "funded_dir = \"/content/drive/MyDrive/3.Google_Colab_repo_11Apr/approved\"\n",
        "rejected_dir = \"/content/drive/MyDrive/3.Google_Colab_repo_11Apr/Rejection\"\n",
        "MODEL_PATH = \"proposal_classifier.pkl\"\n",
        "\n",
        "# === Build Global Retriever (from funded/rejected proposals) ===\n",
        "# This part assumes you've run the initial code to create and save \"grant_faiss_index_with_labels\"\n",
        "global_vectorstore = None\n",
        "global_retriever = None\n",
        "try:\n",
        "    if os.path.exists(\"grant_faiss_index_with_labels\"):\n",
        "        global_vectorstore = FAISS.load_local(\n",
        "            \"grant_faiss_index_with_labels\", embedding_model,\n",
        "            allow_dangerous_deserialization=True\n",
        "        )\n",
        "        global_retriever = global_vectorstore.as_retriever()\n",
        "        print(\"Global FAISS vectorstore (funded/rejected proposals) loaded successfully.\")\n",
        "    else:\n",
        "        print(\"Global FAISS vectorstore 'grant_faiss_index_with_labels' not found. Training classifier might be affected.\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not load global FAISS vectorstore: {e}.\")\n",
        "\n",
        "\n",
        "# === Initialize LLM ===\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "# === Utility for PDF-based retriever ===\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "\n",
        "def create_pdf_retriever(pdf_path, is_default_pdf=False):\n",
        "    \"\"\"Creates a FAISS retriever from a single PDF file.\"\"\"\n",
        "    if not os.path.exists(pdf_path):\n",
        "        print(f\"Error: PDF file not found at {pdf_path}\")\n",
        "        return None\n",
        "    try:\n",
        "        loader = PyPDFLoader(pdf_path)\n",
        "        pages = loader.load()\n",
        "        docs = text_splitter.split_documents(pages)\n",
        "        # Add metadata including source and page number\n",
        "        tagged = [Document(page_content=d.page_content, metadata={\"source\": os.path.basename(pdf_path), \"page\": d.metadata.get(\"page\", \"N/A\")}) for d in docs]\n",
        "        vs = FAISS.from_documents(tagged, embedding_model)\n",
        "        print(f\"Retriever created for {pdf_path} with {len(docs)} chunks.\")\n",
        "        return vs.as_retriever()\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating retriever for {pdf_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# === Train or Load Funding Classifier ===\n",
        "def load_labeled_docs_for_classifier():\n",
        "    \"\"\"Loads and tags documents from funded and rejected directories for classifier training.\"\"\"\n",
        "    tagged_docs = []\n",
        "    def load_split_tag(dir_path, label):\n",
        "        temp_tagged = []\n",
        "        if not os.path.exists(dir_path):\n",
        "            print(f\"Directory not found for classifier training: {dir_path}\")\n",
        "            return []\n",
        "        for fname in os.listdir(dir_path):\n",
        "            if fname.endswith('.pdf'):\n",
        "                full_path = os.path.join(dir_path, fname)\n",
        "                try:\n",
        "                    loader = PyPDFLoader(full_path)\n",
        "                    pages = loader.load()\n",
        "                    chunks = text_splitter.split_documents(pages)\n",
        "                    for c in chunks:\n",
        "                        temp_tagged.append(Document(page_content=c.page_content, metadata={\"label\":label, \"source\": fname}))\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading {full_path} for classifier: {e}\")\n",
        "        return temp_tagged\n",
        "\n",
        "    funded = load_split_tag(funded_dir, \"funded\")\n",
        "    rejected = load_split_tag(rejected_dir, \"rejected\")\n",
        "    tagged_docs.extend(funded)\n",
        "    tagged_docs.extend(rejected)\n",
        "    return tagged_docs\n",
        "\n",
        "classifier = None\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    try:\n",
        "        classifier = joblib.load(MODEL_PATH)\n",
        "        print(\"Classifier loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading classifier: {e}. Retraining...\")\n",
        "        classifier = None # Force retraining if load fails\n",
        "else:\n",
        "    print(\"Classifier model not found. Training new classifier...\")\n",
        "\n",
        "if classifier is None: # Train if not loaded or failed to load\n",
        "    docs_for_clf = load_labeled_docs_for_classifier()\n",
        "    if docs_for_clf:\n",
        "        texts = [d.page_content for d in docs_for_clf]\n",
        "        labels = [1 if d.metadata[\"label\"]==\"funded\" else 0 for d in docs_for_clf]\n",
        "        try:\n",
        "            embeddings = embedding_model.embed_documents(texts)\n",
        "            clf = LogisticRegression(max_iter=1000)\n",
        "            clf.fit(embeddings, labels)\n",
        "            joblib.dump(clf, MODEL_PATH)\n",
        "            classifier = clf\n",
        "            print(\"Classifier trained and saved.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error training classifier: {e}. Prediction functionality may not work.\")\n",
        "    else:\n",
        "        print(\"No documents found for classifier training. Prediction functionality disabled.\")\n",
        "\n",
        "\n",
        "# === Functions ===\n",
        "def answer_question(question, uploaded_pdf_file):\n",
        "    \"\"\"\n",
        "    Answers a question using RAG. Prioritizes sources in this order:\n",
        "    1. User-uploaded PDF.\n",
        "    2. Hardcoded MC_GENAI_STUDY_PDF.\n",
        "    3. Global retriever (from funded/rejected proposals).\n",
        "    \"\"\"\n",
        "    if not question.strip():\n",
        "        return \"Please enter a question.\", \"\"\n",
        "\n",
        "    current_retriever = None\n",
        "    source_description = \"\"\n",
        "\n",
        "    # Priority 1: Use the explicitly uploaded PDF from Gradio\n",
        "    if uploaded_pdf_file and uploaded_pdf_file.name:\n",
        "        current_retriever = create_pdf_retriever(uploaded_pdf_file.name)\n",
        "        source_description = f\"using uploaded PDF: {os.path.basename(uploaded_pdf_file.name)}\"\n",
        "        print(f\"Using retriever for uploaded PDF: {uploaded_pdf_file.name}.\")\n",
        "    # Priority 2: Use the hardcoded MC_GenAI_Study_1 PDF if no other PDF is uploaded\n",
        "    elif os.path.exists(MC_GENAI_STUDY_PDF):\n",
        "        current_retriever = create_pdf_retriever(MC_GENAI_STUDY_PDF)\n",
        "        source_description = f\"using default PDF: {os.path.basename(MC_GENAI_STUDY_PDF)}\"\n",
        "        print(f\"Using retriever for default PDF: {MC_GENAI_STUDY_PDF}.\")\n",
        "    # Priority 3: Fallback to global retriever if no specific PDF is provided\n",
        "    elif global_retriever is not None:\n",
        "        current_retriever = global_retriever\n",
        "        source_description = \"using pre-loaded funded/rejected proposals\"\n",
        "        print(\"Using global retriever (funded/rejected proposals).\")\n",
        "    else:\n",
        "        return \"Error: No document source available to answer the question. Please upload a PDF or ensure the default PDF path is correct and accessible.\", \"\"\n",
        "\n",
        "    if current_retriever is None:\n",
        "        return f\"Error: Could not create a retriever from the specified document source ({source_description}).\", \"\"\n",
        "\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        retriever=current_retriever,\n",
        "        return_source_documents=True\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        result = qa_chain({\"query\": question})\n",
        "        answer = result[\"result\"]\n",
        "        docs = result[\"source_documents\"]\n",
        "        formatted_sources = []\n",
        "        for doc in docs:\n",
        "            src = doc.metadata.get(\"source\", \"N/A\")\n",
        "            page = doc.metadata.get(\"page\", \"N/A\")\n",
        "            snippet = doc.page_content[:200].replace(\"\\n\", \" \") + \"...\"\n",
        "            formatted_sources.append(f\"Source: {src}, Page: {page}: {snippet}\")\n",
        "        return answer, \"\\n\\n\".join(formatted_sources)\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred during answering from {source_description}: {e}\", \"\"\n",
        "\n",
        "\n",
        "def predict_likelihood(pdf_file):\n",
        "    \"\"\"Predicts the funding likelihood of an uploaded PDF using the trained classifier.\"\"\"\n",
        "    if not classifier:\n",
        "        return \"Classifier not available. Please ensure training data directories are correctly configured and contain PDFs.\"\n",
        "\n",
        "    if not pdf_file:\n",
        "        return \"Upload a PDF file to predict likelihood.\"\n",
        "\n",
        "    try:\n",
        "        loader = PyPDFLoader(pdf_file.name)\n",
        "        pages = loader.load()\n",
        "        chunks = text_splitter.split_documents(pages)\n",
        "        if not chunks:\n",
        "            return \"No text could be extracted from the uploaded PDF.\"\n",
        "\n",
        "        texts = [c.page_content for c in chunks]\n",
        "        embeddings = embedding_model.embed_documents(texts)\n",
        "        probs = classifier.predict_proba(embeddings)[:,1]\n",
        "        avg_prob = float(probs.mean())\n",
        "        return f\"Estimated funding probability: {avg_prob*100:.2f}%\"\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred during prediction: {e}\"\n",
        "\n",
        "# Combined chat+predict function for UI\n",
        "def chat_and_predict(question, pdf_file):\n",
        "    answer, sources = answer_question(question, pdf_file)\n",
        "    # Only predict likelihood if a PDF was specifically uploaded for chat\n",
        "    likelihood = predict_likelihood(pdf_file) if pdf_file else \"Upload a PDF for prediction in this tab.\"\n",
        "    return answer, sources, likelihood\n",
        "\n",
        "# === Gradio Interface ===\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Grant Proposal Reviewer & Success Predictor Chatbot\")\n",
        "    gr.Markdown(\n",
        "        f\"Ask questions about grant proposals. \"\n",
        "        f\"By default, the chatbot will answer questions based on **'{os.path.basename(MC_GENAI_STUDY_PDF)}'** if no other PDF is uploaded. \"\n",
        "        \"You can also upload a different PDF to chat with it, or use the 'Predict Funding' tab to assess funding likelihood of a proposal.\"\n",
        "    )\n",
        "    with gr.Tab(\"Chat & Search\"):\n",
        "        inp = gr.Textbox(label=\"Your Question\", placeholder=\"Ask about the economic potential of GenAI or upload a proposal...\")\n",
        "        pdf_q = gr.File(label=\"(Optional) Upload a PDF to chat with it (overrides default)\", file_types=[\".pdf\"])\n",
        "        btn_q = gr.Button(\"Submit Query\")\n",
        "        out_answer = gr.Textbox(label=\"AI Answer\", interactive=False, lines=5)\n",
        "        out_sources = gr.Textbox(label=\"Source Snippets\", interactive=False, lines=8)\n",
        "        out_prob_chat = gr.Textbox(label=\"Funding Likelihood (for uploaded PDF)\", interactive=False)\n",
        "        btn_q.click(fn=chat_and_predict, inputs=[inp, pdf_q], outputs=[out_answer, out_sources, out_prob_chat])\n",
        "    with gr.Tab(\"Predict Funding\"):\n",
        "        pdf_p = gr.File(label=\"Upload PDF to Predict Funding\", file_types=[\".pdf\"])\n",
        "        btn_p = gr.Button(\"Predict Likelihood\")\n",
        "        out_prob = gr.Textbox(label=\"Funding Probability\", interactive=False)\n",
        "        btn_p.click(fn=predict_likelihood, inputs=pdf_p, outputs=out_prob)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(share=True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "C81Hp5bnu-kt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}